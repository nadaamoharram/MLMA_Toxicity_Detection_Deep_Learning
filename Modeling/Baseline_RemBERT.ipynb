{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "borrowed from bella's version by her suggestion"
      ],
      "metadata": {
        "id": "MEsmk18276Cy"
      },
      "id": "MEsmk18276Cy"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Ytpr43FFf55C",
      "metadata": {
        "id": "Ytpr43FFf55C"
      },
      "outputs": [],
      "source": [
        "!pip install -q evaluate torchinfo bitsandbytes peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5f3c4607",
      "metadata": {
        "id": "5f3c4607"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from torchinfo import summary\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BitsAndBytesConfig\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "e8g0jfIP8TCN"
      },
      "id": "e8g0jfIP8TCN",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "PAGKhUDbf-6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAGKhUDbf-6c",
        "outputId": "a81c46f1-de27-4df4-d70a-2bb90a216f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ec3af9",
      "metadata": {
        "id": "88ec3af9"
      },
      "source": [
        "## Load Datasets, Tokenizer, and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f1870343",
      "metadata": {
        "id": "f1870343"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/266/MLMA_Toxicity_Detection_Deep_Learning/Data/combined_train.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/266/MLMA_Toxicity_Detection_Deep_Learning/Data/combined_val.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/266/MLMA_Toxicity_Detection_Deep_Learning/Data/combined_test.csv')\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0fHEJTkwgwAc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fHEJTkwgwAc",
        "outputId": "7a1517b0-7583-42fb-fda4-1b67dc16d080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('google/rembert')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('google/rembert')\n",
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9c7ca899",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f9c625d67645461b8091bcdcb2163c63",
            "e0e4046d593c4beeb20fc95006b937f4",
            "1862d5d1184840f785764c4f7a495bba",
            "e94de0dda93b4b78a1dda3a23cb88b73",
            "6990060972504539bfffeca0d73ee1ba",
            "ad7a77b4cbe3422ba0462ea6883464e5",
            "1912db96fdfc4d779fb5af692ad0c13d",
            "de5d83aaa0814f3195bc9e89f9b0ab3d",
            "8dff4a2dd3d549f1a01e812baa248aa8",
            "64b2426a1919496185189f618f897abd",
            "9a037da8941d4f7588ee8cd1ec9caa52",
            "7a69b75bb08b4105a6d80001f05a8bef",
            "3333f29358af4ae58e496be16e81675a",
            "2c1f2590d17849bcb44493af51f8fa70",
            "0024d592205c4f908a4d7def0a05d31e",
            "4883242d6af14ad18700551be6112f4b",
            "13d6ea4a1e0c453d81d7ce9d758106ab",
            "de7ceda7db26473a93ddd5300e2aa602",
            "a51c7b39ca50452f8e1da11149fbb6d7",
            "4b729ca6a1ca47629751b00e3a3ead8c",
            "d39b58f10b124979899dcd3b4e34bee8",
            "48d8a25eb8184c60a3b49da2b851287e"
          ]
        },
        "id": "9c7ca899",
        "outputId": "ff4258dd-627f-44fe-e359-56bffe7e4538"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/30702 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9c625d67645461b8091bcdcb2163c63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3901 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a69b75bb08b4105a6d80001f05a8bef"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def preprocess(batch):\n",
        "    # Ensure all entries are strings and handle missing values\n",
        "    review_texts = [str(x) for x in batch['text_cleaned']]\n",
        "    encoded = tokenizer(\n",
        "        review_texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=64,\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Return input features and the label\n",
        "    return {\n",
        "        'input_ids': encoded['input_ids'],\n",
        "        'token_type_ids': encoded['token_type_ids'],\n",
        "        'attention_mask': encoded['attention_mask'],\n",
        "        'labels': batch['hatespeech'] # Keep the 'hatespeech' column as labels\n",
        "    }\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess, batched=True)\n",
        "\n",
        "# Identify columns to remove (all except the new ones)\n",
        "original_columns = train_df.columns.tolist()\n",
        "columns_to_keep = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n",
        "columns_to_remove = [col for col in original_columns if col not in columns_to_keep]\n",
        "\n",
        "# Remove original columns from the datasets\n",
        "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
        "val_dataset = val_dataset.remove_columns(columns_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a039f2f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a039f2f4",
        "outputId": "46bc644c-65c9-48ca-bd27-320780bb743d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 30702\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "285c3375",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "285c3375",
        "outputId": "10a9f22a-5a72-4307-a778-eb6c024b16e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "هههه وانت لا تنسى ان تمسح بصقة اليهودي الذي إستباح ارضك وعِرضك من....... اما بول البعير ياجاهل فهو فخر ل…\n",
            "{'input_ids': tensor([[   312,  20236,  36763,    717,  36367,   3698,  13916,  85378,   2459,\n",
            "           6971, 112100, 113172,  32804,    573, 181206, 122607,  10624, 134299,\n",
            "          93834,  17442,    573, 220069,   2262,  39649,   1332,  65420,   2262,\n",
            "           1109,  54673,  14963,  69486,    978,  55184,   9292,  14900,  23464,\n",
            "          58995,    573, 202668,   2598,  41749,   1687,    655,    313]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "SequenceClassifierOutput(loss=None, logits=tensor([[-0.3358, -0.2998]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "# Show a sample input and output\n",
        "\n",
        "text = train_df.iloc[5000]['text_cleaned']\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = model.device\n",
        "encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "\n",
        "output = model(**encoded_input)\n",
        "\n",
        "print(text)\n",
        "print(encoded_input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd884285",
      "metadata": {
        "id": "fd884285"
      },
      "source": [
        "## Model Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e2f564f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f564f6",
        "outputId": "53bdf5c8-a5b0-4bf1-8622-db84e2a9faf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rembert.embeddings.word_embeddings.weight\n",
            "rembert.embeddings.position_embeddings.weight\n",
            "rembert.embeddings.token_type_embeddings.weight\n",
            "rembert.embeddings.LayerNorm.weight\n",
            "rembert.embeddings.LayerNorm.bias\n",
            "rembert.encoder.embedding_hidden_mapping_in.weight\n",
            "rembert.encoder.embedding_hidden_mapping_in.bias\n",
            "rembert.encoder.layer.0.attention.self.query.weight\n",
            "rembert.encoder.layer.0.attention.self.query.bias\n",
            "rembert.encoder.layer.0.attention.self.key.weight\n",
            "rembert.encoder.layer.0.attention.self.key.bias\n",
            "rembert.encoder.layer.0.attention.self.value.weight\n",
            "rembert.encoder.layer.0.attention.self.value.bias\n",
            "rembert.encoder.layer.0.attention.output.dense.weight\n",
            "rembert.encoder.layer.0.attention.output.dense.bias\n",
            "rembert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.0.intermediate.dense.weight\n",
            "rembert.encoder.layer.0.intermediate.dense.bias\n",
            "rembert.encoder.layer.0.output.dense.weight\n",
            "rembert.encoder.layer.0.output.dense.bias\n",
            "rembert.encoder.layer.0.output.LayerNorm.weight\n",
            "rembert.encoder.layer.0.output.LayerNorm.bias\n",
            "rembert.encoder.layer.1.attention.self.query.weight\n",
            "rembert.encoder.layer.1.attention.self.query.bias\n",
            "rembert.encoder.layer.1.attention.self.key.weight\n",
            "rembert.encoder.layer.1.attention.self.key.bias\n",
            "rembert.encoder.layer.1.attention.self.value.weight\n",
            "rembert.encoder.layer.1.attention.self.value.bias\n",
            "rembert.encoder.layer.1.attention.output.dense.weight\n",
            "rembert.encoder.layer.1.attention.output.dense.bias\n",
            "rembert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.1.intermediate.dense.weight\n",
            "rembert.encoder.layer.1.intermediate.dense.bias\n",
            "rembert.encoder.layer.1.output.dense.weight\n",
            "rembert.encoder.layer.1.output.dense.bias\n",
            "rembert.encoder.layer.1.output.LayerNorm.weight\n",
            "rembert.encoder.layer.1.output.LayerNorm.bias\n",
            "rembert.encoder.layer.2.attention.self.query.weight\n",
            "rembert.encoder.layer.2.attention.self.query.bias\n",
            "rembert.encoder.layer.2.attention.self.key.weight\n",
            "rembert.encoder.layer.2.attention.self.key.bias\n",
            "rembert.encoder.layer.2.attention.self.value.weight\n",
            "rembert.encoder.layer.2.attention.self.value.bias\n",
            "rembert.encoder.layer.2.attention.output.dense.weight\n",
            "rembert.encoder.layer.2.attention.output.dense.bias\n",
            "rembert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.2.intermediate.dense.weight\n",
            "rembert.encoder.layer.2.intermediate.dense.bias\n",
            "rembert.encoder.layer.2.output.dense.weight\n",
            "rembert.encoder.layer.2.output.dense.bias\n",
            "rembert.encoder.layer.2.output.LayerNorm.weight\n",
            "rembert.encoder.layer.2.output.LayerNorm.bias\n",
            "rembert.encoder.layer.3.attention.self.query.weight\n",
            "rembert.encoder.layer.3.attention.self.query.bias\n",
            "rembert.encoder.layer.3.attention.self.key.weight\n",
            "rembert.encoder.layer.3.attention.self.key.bias\n",
            "rembert.encoder.layer.3.attention.self.value.weight\n",
            "rembert.encoder.layer.3.attention.self.value.bias\n",
            "rembert.encoder.layer.3.attention.output.dense.weight\n",
            "rembert.encoder.layer.3.attention.output.dense.bias\n",
            "rembert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.3.intermediate.dense.weight\n",
            "rembert.encoder.layer.3.intermediate.dense.bias\n",
            "rembert.encoder.layer.3.output.dense.weight\n",
            "rembert.encoder.layer.3.output.dense.bias\n",
            "rembert.encoder.layer.3.output.LayerNorm.weight\n",
            "rembert.encoder.layer.3.output.LayerNorm.bias\n",
            "rembert.encoder.layer.4.attention.self.query.weight\n",
            "rembert.encoder.layer.4.attention.self.query.bias\n",
            "rembert.encoder.layer.4.attention.self.key.weight\n",
            "rembert.encoder.layer.4.attention.self.key.bias\n",
            "rembert.encoder.layer.4.attention.self.value.weight\n",
            "rembert.encoder.layer.4.attention.self.value.bias\n",
            "rembert.encoder.layer.4.attention.output.dense.weight\n",
            "rembert.encoder.layer.4.attention.output.dense.bias\n",
            "rembert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.4.intermediate.dense.weight\n",
            "rembert.encoder.layer.4.intermediate.dense.bias\n",
            "rembert.encoder.layer.4.output.dense.weight\n",
            "rembert.encoder.layer.4.output.dense.bias\n",
            "rembert.encoder.layer.4.output.LayerNorm.weight\n",
            "rembert.encoder.layer.4.output.LayerNorm.bias\n",
            "rembert.encoder.layer.5.attention.self.query.weight\n",
            "rembert.encoder.layer.5.attention.self.query.bias\n",
            "rembert.encoder.layer.5.attention.self.key.weight\n",
            "rembert.encoder.layer.5.attention.self.key.bias\n",
            "rembert.encoder.layer.5.attention.self.value.weight\n",
            "rembert.encoder.layer.5.attention.self.value.bias\n",
            "rembert.encoder.layer.5.attention.output.dense.weight\n",
            "rembert.encoder.layer.5.attention.output.dense.bias\n",
            "rembert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.5.intermediate.dense.weight\n",
            "rembert.encoder.layer.5.intermediate.dense.bias\n",
            "rembert.encoder.layer.5.output.dense.weight\n",
            "rembert.encoder.layer.5.output.dense.bias\n",
            "rembert.encoder.layer.5.output.LayerNorm.weight\n",
            "rembert.encoder.layer.5.output.LayerNorm.bias\n",
            "rembert.encoder.layer.6.attention.self.query.weight\n",
            "rembert.encoder.layer.6.attention.self.query.bias\n",
            "rembert.encoder.layer.6.attention.self.key.weight\n",
            "rembert.encoder.layer.6.attention.self.key.bias\n",
            "rembert.encoder.layer.6.attention.self.value.weight\n",
            "rembert.encoder.layer.6.attention.self.value.bias\n",
            "rembert.encoder.layer.6.attention.output.dense.weight\n",
            "rembert.encoder.layer.6.attention.output.dense.bias\n",
            "rembert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.6.intermediate.dense.weight\n",
            "rembert.encoder.layer.6.intermediate.dense.bias\n",
            "rembert.encoder.layer.6.output.dense.weight\n",
            "rembert.encoder.layer.6.output.dense.bias\n",
            "rembert.encoder.layer.6.output.LayerNorm.weight\n",
            "rembert.encoder.layer.6.output.LayerNorm.bias\n",
            "rembert.encoder.layer.7.attention.self.query.weight\n",
            "rembert.encoder.layer.7.attention.self.query.bias\n",
            "rembert.encoder.layer.7.attention.self.key.weight\n",
            "rembert.encoder.layer.7.attention.self.key.bias\n",
            "rembert.encoder.layer.7.attention.self.value.weight\n",
            "rembert.encoder.layer.7.attention.self.value.bias\n",
            "rembert.encoder.layer.7.attention.output.dense.weight\n",
            "rembert.encoder.layer.7.attention.output.dense.bias\n",
            "rembert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.7.intermediate.dense.weight\n",
            "rembert.encoder.layer.7.intermediate.dense.bias\n",
            "rembert.encoder.layer.7.output.dense.weight\n",
            "rembert.encoder.layer.7.output.dense.bias\n",
            "rembert.encoder.layer.7.output.LayerNorm.weight\n",
            "rembert.encoder.layer.7.output.LayerNorm.bias\n",
            "rembert.encoder.layer.8.attention.self.query.weight\n",
            "rembert.encoder.layer.8.attention.self.query.bias\n",
            "rembert.encoder.layer.8.attention.self.key.weight\n",
            "rembert.encoder.layer.8.attention.self.key.bias\n",
            "rembert.encoder.layer.8.attention.self.value.weight\n",
            "rembert.encoder.layer.8.attention.self.value.bias\n",
            "rembert.encoder.layer.8.attention.output.dense.weight\n",
            "rembert.encoder.layer.8.attention.output.dense.bias\n",
            "rembert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.8.intermediate.dense.weight\n",
            "rembert.encoder.layer.8.intermediate.dense.bias\n",
            "rembert.encoder.layer.8.output.dense.weight\n",
            "rembert.encoder.layer.8.output.dense.bias\n",
            "rembert.encoder.layer.8.output.LayerNorm.weight\n",
            "rembert.encoder.layer.8.output.LayerNorm.bias\n",
            "rembert.encoder.layer.9.attention.self.query.weight\n",
            "rembert.encoder.layer.9.attention.self.query.bias\n",
            "rembert.encoder.layer.9.attention.self.key.weight\n",
            "rembert.encoder.layer.9.attention.self.key.bias\n",
            "rembert.encoder.layer.9.attention.self.value.weight\n",
            "rembert.encoder.layer.9.attention.self.value.bias\n",
            "rembert.encoder.layer.9.attention.output.dense.weight\n",
            "rembert.encoder.layer.9.attention.output.dense.bias\n",
            "rembert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.9.intermediate.dense.weight\n",
            "rembert.encoder.layer.9.intermediate.dense.bias\n",
            "rembert.encoder.layer.9.output.dense.weight\n",
            "rembert.encoder.layer.9.output.dense.bias\n",
            "rembert.encoder.layer.9.output.LayerNorm.weight\n",
            "rembert.encoder.layer.9.output.LayerNorm.bias\n",
            "rembert.encoder.layer.10.attention.self.query.weight\n",
            "rembert.encoder.layer.10.attention.self.query.bias\n",
            "rembert.encoder.layer.10.attention.self.key.weight\n",
            "rembert.encoder.layer.10.attention.self.key.bias\n",
            "rembert.encoder.layer.10.attention.self.value.weight\n",
            "rembert.encoder.layer.10.attention.self.value.bias\n",
            "rembert.encoder.layer.10.attention.output.dense.weight\n",
            "rembert.encoder.layer.10.attention.output.dense.bias\n",
            "rembert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.10.intermediate.dense.weight\n",
            "rembert.encoder.layer.10.intermediate.dense.bias\n",
            "rembert.encoder.layer.10.output.dense.weight\n",
            "rembert.encoder.layer.10.output.dense.bias\n",
            "rembert.encoder.layer.10.output.LayerNorm.weight\n",
            "rembert.encoder.layer.10.output.LayerNorm.bias\n",
            "rembert.encoder.layer.11.attention.self.query.weight\n",
            "rembert.encoder.layer.11.attention.self.query.bias\n",
            "rembert.encoder.layer.11.attention.self.key.weight\n",
            "rembert.encoder.layer.11.attention.self.key.bias\n",
            "rembert.encoder.layer.11.attention.self.value.weight\n",
            "rembert.encoder.layer.11.attention.self.value.bias\n",
            "rembert.encoder.layer.11.attention.output.dense.weight\n",
            "rembert.encoder.layer.11.attention.output.dense.bias\n",
            "rembert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.11.intermediate.dense.weight\n",
            "rembert.encoder.layer.11.intermediate.dense.bias\n",
            "rembert.encoder.layer.11.output.dense.weight\n",
            "rembert.encoder.layer.11.output.dense.bias\n",
            "rembert.encoder.layer.11.output.LayerNorm.weight\n",
            "rembert.encoder.layer.11.output.LayerNorm.bias\n",
            "rembert.encoder.layer.12.attention.self.query.weight\n",
            "rembert.encoder.layer.12.attention.self.query.bias\n",
            "rembert.encoder.layer.12.attention.self.key.weight\n",
            "rembert.encoder.layer.12.attention.self.key.bias\n",
            "rembert.encoder.layer.12.attention.self.value.weight\n",
            "rembert.encoder.layer.12.attention.self.value.bias\n",
            "rembert.encoder.layer.12.attention.output.dense.weight\n",
            "rembert.encoder.layer.12.attention.output.dense.bias\n",
            "rembert.encoder.layer.12.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.12.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.12.intermediate.dense.weight\n",
            "rembert.encoder.layer.12.intermediate.dense.bias\n",
            "rembert.encoder.layer.12.output.dense.weight\n",
            "rembert.encoder.layer.12.output.dense.bias\n",
            "rembert.encoder.layer.12.output.LayerNorm.weight\n",
            "rembert.encoder.layer.12.output.LayerNorm.bias\n",
            "rembert.encoder.layer.13.attention.self.query.weight\n",
            "rembert.encoder.layer.13.attention.self.query.bias\n",
            "rembert.encoder.layer.13.attention.self.key.weight\n",
            "rembert.encoder.layer.13.attention.self.key.bias\n",
            "rembert.encoder.layer.13.attention.self.value.weight\n",
            "rembert.encoder.layer.13.attention.self.value.bias\n",
            "rembert.encoder.layer.13.attention.output.dense.weight\n",
            "rembert.encoder.layer.13.attention.output.dense.bias\n",
            "rembert.encoder.layer.13.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.13.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.13.intermediate.dense.weight\n",
            "rembert.encoder.layer.13.intermediate.dense.bias\n",
            "rembert.encoder.layer.13.output.dense.weight\n",
            "rembert.encoder.layer.13.output.dense.bias\n",
            "rembert.encoder.layer.13.output.LayerNorm.weight\n",
            "rembert.encoder.layer.13.output.LayerNorm.bias\n",
            "rembert.encoder.layer.14.attention.self.query.weight\n",
            "rembert.encoder.layer.14.attention.self.query.bias\n",
            "rembert.encoder.layer.14.attention.self.key.weight\n",
            "rembert.encoder.layer.14.attention.self.key.bias\n",
            "rembert.encoder.layer.14.attention.self.value.weight\n",
            "rembert.encoder.layer.14.attention.self.value.bias\n",
            "rembert.encoder.layer.14.attention.output.dense.weight\n",
            "rembert.encoder.layer.14.attention.output.dense.bias\n",
            "rembert.encoder.layer.14.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.14.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.14.intermediate.dense.weight\n",
            "rembert.encoder.layer.14.intermediate.dense.bias\n",
            "rembert.encoder.layer.14.output.dense.weight\n",
            "rembert.encoder.layer.14.output.dense.bias\n",
            "rembert.encoder.layer.14.output.LayerNorm.weight\n",
            "rembert.encoder.layer.14.output.LayerNorm.bias\n",
            "rembert.encoder.layer.15.attention.self.query.weight\n",
            "rembert.encoder.layer.15.attention.self.query.bias\n",
            "rembert.encoder.layer.15.attention.self.key.weight\n",
            "rembert.encoder.layer.15.attention.self.key.bias\n",
            "rembert.encoder.layer.15.attention.self.value.weight\n",
            "rembert.encoder.layer.15.attention.self.value.bias\n",
            "rembert.encoder.layer.15.attention.output.dense.weight\n",
            "rembert.encoder.layer.15.attention.output.dense.bias\n",
            "rembert.encoder.layer.15.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.15.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.15.intermediate.dense.weight\n",
            "rembert.encoder.layer.15.intermediate.dense.bias\n",
            "rembert.encoder.layer.15.output.dense.weight\n",
            "rembert.encoder.layer.15.output.dense.bias\n",
            "rembert.encoder.layer.15.output.LayerNorm.weight\n",
            "rembert.encoder.layer.15.output.LayerNorm.bias\n",
            "rembert.encoder.layer.16.attention.self.query.weight\n",
            "rembert.encoder.layer.16.attention.self.query.bias\n",
            "rembert.encoder.layer.16.attention.self.key.weight\n",
            "rembert.encoder.layer.16.attention.self.key.bias\n",
            "rembert.encoder.layer.16.attention.self.value.weight\n",
            "rembert.encoder.layer.16.attention.self.value.bias\n",
            "rembert.encoder.layer.16.attention.output.dense.weight\n",
            "rembert.encoder.layer.16.attention.output.dense.bias\n",
            "rembert.encoder.layer.16.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.16.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.16.intermediate.dense.weight\n",
            "rembert.encoder.layer.16.intermediate.dense.bias\n",
            "rembert.encoder.layer.16.output.dense.weight\n",
            "rembert.encoder.layer.16.output.dense.bias\n",
            "rembert.encoder.layer.16.output.LayerNorm.weight\n",
            "rembert.encoder.layer.16.output.LayerNorm.bias\n",
            "rembert.encoder.layer.17.attention.self.query.weight\n",
            "rembert.encoder.layer.17.attention.self.query.bias\n",
            "rembert.encoder.layer.17.attention.self.key.weight\n",
            "rembert.encoder.layer.17.attention.self.key.bias\n",
            "rembert.encoder.layer.17.attention.self.value.weight\n",
            "rembert.encoder.layer.17.attention.self.value.bias\n",
            "rembert.encoder.layer.17.attention.output.dense.weight\n",
            "rembert.encoder.layer.17.attention.output.dense.bias\n",
            "rembert.encoder.layer.17.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.17.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.17.intermediate.dense.weight\n",
            "rembert.encoder.layer.17.intermediate.dense.bias\n",
            "rembert.encoder.layer.17.output.dense.weight\n",
            "rembert.encoder.layer.17.output.dense.bias\n",
            "rembert.encoder.layer.17.output.LayerNorm.weight\n",
            "rembert.encoder.layer.17.output.LayerNorm.bias\n",
            "rembert.encoder.layer.18.attention.self.query.weight\n",
            "rembert.encoder.layer.18.attention.self.query.bias\n",
            "rembert.encoder.layer.18.attention.self.key.weight\n",
            "rembert.encoder.layer.18.attention.self.key.bias\n",
            "rembert.encoder.layer.18.attention.self.value.weight\n",
            "rembert.encoder.layer.18.attention.self.value.bias\n",
            "rembert.encoder.layer.18.attention.output.dense.weight\n",
            "rembert.encoder.layer.18.attention.output.dense.bias\n",
            "rembert.encoder.layer.18.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.18.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.18.intermediate.dense.weight\n",
            "rembert.encoder.layer.18.intermediate.dense.bias\n",
            "rembert.encoder.layer.18.output.dense.weight\n",
            "rembert.encoder.layer.18.output.dense.bias\n",
            "rembert.encoder.layer.18.output.LayerNorm.weight\n",
            "rembert.encoder.layer.18.output.LayerNorm.bias\n",
            "rembert.encoder.layer.19.attention.self.query.weight\n",
            "rembert.encoder.layer.19.attention.self.query.bias\n",
            "rembert.encoder.layer.19.attention.self.key.weight\n",
            "rembert.encoder.layer.19.attention.self.key.bias\n",
            "rembert.encoder.layer.19.attention.self.value.weight\n",
            "rembert.encoder.layer.19.attention.self.value.bias\n",
            "rembert.encoder.layer.19.attention.output.dense.weight\n",
            "rembert.encoder.layer.19.attention.output.dense.bias\n",
            "rembert.encoder.layer.19.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.19.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.19.intermediate.dense.weight\n",
            "rembert.encoder.layer.19.intermediate.dense.bias\n",
            "rembert.encoder.layer.19.output.dense.weight\n",
            "rembert.encoder.layer.19.output.dense.bias\n",
            "rembert.encoder.layer.19.output.LayerNorm.weight\n",
            "rembert.encoder.layer.19.output.LayerNorm.bias\n",
            "rembert.encoder.layer.20.attention.self.query.weight\n",
            "rembert.encoder.layer.20.attention.self.query.bias\n",
            "rembert.encoder.layer.20.attention.self.key.weight\n",
            "rembert.encoder.layer.20.attention.self.key.bias\n",
            "rembert.encoder.layer.20.attention.self.value.weight\n",
            "rembert.encoder.layer.20.attention.self.value.bias\n",
            "rembert.encoder.layer.20.attention.output.dense.weight\n",
            "rembert.encoder.layer.20.attention.output.dense.bias\n",
            "rembert.encoder.layer.20.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.20.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.20.intermediate.dense.weight\n",
            "rembert.encoder.layer.20.intermediate.dense.bias\n",
            "rembert.encoder.layer.20.output.dense.weight\n",
            "rembert.encoder.layer.20.output.dense.bias\n",
            "rembert.encoder.layer.20.output.LayerNorm.weight\n",
            "rembert.encoder.layer.20.output.LayerNorm.bias\n",
            "rembert.encoder.layer.21.attention.self.query.weight\n",
            "rembert.encoder.layer.21.attention.self.query.bias\n",
            "rembert.encoder.layer.21.attention.self.key.weight\n",
            "rembert.encoder.layer.21.attention.self.key.bias\n",
            "rembert.encoder.layer.21.attention.self.value.weight\n",
            "rembert.encoder.layer.21.attention.self.value.bias\n",
            "rembert.encoder.layer.21.attention.output.dense.weight\n",
            "rembert.encoder.layer.21.attention.output.dense.bias\n",
            "rembert.encoder.layer.21.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.21.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.21.intermediate.dense.weight\n",
            "rembert.encoder.layer.21.intermediate.dense.bias\n",
            "rembert.encoder.layer.21.output.dense.weight\n",
            "rembert.encoder.layer.21.output.dense.bias\n",
            "rembert.encoder.layer.21.output.LayerNorm.weight\n",
            "rembert.encoder.layer.21.output.LayerNorm.bias\n",
            "rembert.encoder.layer.22.attention.self.query.weight\n",
            "rembert.encoder.layer.22.attention.self.query.bias\n",
            "rembert.encoder.layer.22.attention.self.key.weight\n",
            "rembert.encoder.layer.22.attention.self.key.bias\n",
            "rembert.encoder.layer.22.attention.self.value.weight\n",
            "rembert.encoder.layer.22.attention.self.value.bias\n",
            "rembert.encoder.layer.22.attention.output.dense.weight\n",
            "rembert.encoder.layer.22.attention.output.dense.bias\n",
            "rembert.encoder.layer.22.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.22.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.22.intermediate.dense.weight\n",
            "rembert.encoder.layer.22.intermediate.dense.bias\n",
            "rembert.encoder.layer.22.output.dense.weight\n",
            "rembert.encoder.layer.22.output.dense.bias\n",
            "rembert.encoder.layer.22.output.LayerNorm.weight\n",
            "rembert.encoder.layer.22.output.LayerNorm.bias\n",
            "rembert.encoder.layer.23.attention.self.query.weight\n",
            "rembert.encoder.layer.23.attention.self.query.bias\n",
            "rembert.encoder.layer.23.attention.self.key.weight\n",
            "rembert.encoder.layer.23.attention.self.key.bias\n",
            "rembert.encoder.layer.23.attention.self.value.weight\n",
            "rembert.encoder.layer.23.attention.self.value.bias\n",
            "rembert.encoder.layer.23.attention.output.dense.weight\n",
            "rembert.encoder.layer.23.attention.output.dense.bias\n",
            "rembert.encoder.layer.23.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.23.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.23.intermediate.dense.weight\n",
            "rembert.encoder.layer.23.intermediate.dense.bias\n",
            "rembert.encoder.layer.23.output.dense.weight\n",
            "rembert.encoder.layer.23.output.dense.bias\n",
            "rembert.encoder.layer.23.output.LayerNorm.weight\n",
            "rembert.encoder.layer.23.output.LayerNorm.bias\n",
            "rembert.encoder.layer.24.attention.self.query.weight\n",
            "rembert.encoder.layer.24.attention.self.query.bias\n",
            "rembert.encoder.layer.24.attention.self.key.weight\n",
            "rembert.encoder.layer.24.attention.self.key.bias\n",
            "rembert.encoder.layer.24.attention.self.value.weight\n",
            "rembert.encoder.layer.24.attention.self.value.bias\n",
            "rembert.encoder.layer.24.attention.output.dense.weight\n",
            "rembert.encoder.layer.24.attention.output.dense.bias\n",
            "rembert.encoder.layer.24.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.24.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.24.intermediate.dense.weight\n",
            "rembert.encoder.layer.24.intermediate.dense.bias\n",
            "rembert.encoder.layer.24.output.dense.weight\n",
            "rembert.encoder.layer.24.output.dense.bias\n",
            "rembert.encoder.layer.24.output.LayerNorm.weight\n",
            "rembert.encoder.layer.24.output.LayerNorm.bias\n",
            "rembert.encoder.layer.25.attention.self.query.weight\n",
            "rembert.encoder.layer.25.attention.self.query.bias\n",
            "rembert.encoder.layer.25.attention.self.key.weight\n",
            "rembert.encoder.layer.25.attention.self.key.bias\n",
            "rembert.encoder.layer.25.attention.self.value.weight\n",
            "rembert.encoder.layer.25.attention.self.value.bias\n",
            "rembert.encoder.layer.25.attention.output.dense.weight\n",
            "rembert.encoder.layer.25.attention.output.dense.bias\n",
            "rembert.encoder.layer.25.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.25.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.25.intermediate.dense.weight\n",
            "rembert.encoder.layer.25.intermediate.dense.bias\n",
            "rembert.encoder.layer.25.output.dense.weight\n",
            "rembert.encoder.layer.25.output.dense.bias\n",
            "rembert.encoder.layer.25.output.LayerNorm.weight\n",
            "rembert.encoder.layer.25.output.LayerNorm.bias\n",
            "rembert.encoder.layer.26.attention.self.query.weight\n",
            "rembert.encoder.layer.26.attention.self.query.bias\n",
            "rembert.encoder.layer.26.attention.self.key.weight\n",
            "rembert.encoder.layer.26.attention.self.key.bias\n",
            "rembert.encoder.layer.26.attention.self.value.weight\n",
            "rembert.encoder.layer.26.attention.self.value.bias\n",
            "rembert.encoder.layer.26.attention.output.dense.weight\n",
            "rembert.encoder.layer.26.attention.output.dense.bias\n",
            "rembert.encoder.layer.26.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.26.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.26.intermediate.dense.weight\n",
            "rembert.encoder.layer.26.intermediate.dense.bias\n",
            "rembert.encoder.layer.26.output.dense.weight\n",
            "rembert.encoder.layer.26.output.dense.bias\n",
            "rembert.encoder.layer.26.output.LayerNorm.weight\n",
            "rembert.encoder.layer.26.output.LayerNorm.bias\n",
            "rembert.encoder.layer.27.attention.self.query.weight\n",
            "rembert.encoder.layer.27.attention.self.query.bias\n",
            "rembert.encoder.layer.27.attention.self.key.weight\n",
            "rembert.encoder.layer.27.attention.self.key.bias\n",
            "rembert.encoder.layer.27.attention.self.value.weight\n",
            "rembert.encoder.layer.27.attention.self.value.bias\n",
            "rembert.encoder.layer.27.attention.output.dense.weight\n",
            "rembert.encoder.layer.27.attention.output.dense.bias\n",
            "rembert.encoder.layer.27.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.27.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.27.intermediate.dense.weight\n",
            "rembert.encoder.layer.27.intermediate.dense.bias\n",
            "rembert.encoder.layer.27.output.dense.weight\n",
            "rembert.encoder.layer.27.output.dense.bias\n",
            "rembert.encoder.layer.27.output.LayerNorm.weight\n",
            "rembert.encoder.layer.27.output.LayerNorm.bias\n",
            "rembert.encoder.layer.28.attention.self.query.weight\n",
            "rembert.encoder.layer.28.attention.self.query.bias\n",
            "rembert.encoder.layer.28.attention.self.key.weight\n",
            "rembert.encoder.layer.28.attention.self.key.bias\n",
            "rembert.encoder.layer.28.attention.self.value.weight\n",
            "rembert.encoder.layer.28.attention.self.value.bias\n",
            "rembert.encoder.layer.28.attention.output.dense.weight\n",
            "rembert.encoder.layer.28.attention.output.dense.bias\n",
            "rembert.encoder.layer.28.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.28.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.28.intermediate.dense.weight\n",
            "rembert.encoder.layer.28.intermediate.dense.bias\n",
            "rembert.encoder.layer.28.output.dense.weight\n",
            "rembert.encoder.layer.28.output.dense.bias\n",
            "rembert.encoder.layer.28.output.LayerNorm.weight\n",
            "rembert.encoder.layer.28.output.LayerNorm.bias\n",
            "rembert.encoder.layer.29.attention.self.query.weight\n",
            "rembert.encoder.layer.29.attention.self.query.bias\n",
            "rembert.encoder.layer.29.attention.self.key.weight\n",
            "rembert.encoder.layer.29.attention.self.key.bias\n",
            "rembert.encoder.layer.29.attention.self.value.weight\n",
            "rembert.encoder.layer.29.attention.self.value.bias\n",
            "rembert.encoder.layer.29.attention.output.dense.weight\n",
            "rembert.encoder.layer.29.attention.output.dense.bias\n",
            "rembert.encoder.layer.29.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.29.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.29.intermediate.dense.weight\n",
            "rembert.encoder.layer.29.intermediate.dense.bias\n",
            "rembert.encoder.layer.29.output.dense.weight\n",
            "rembert.encoder.layer.29.output.dense.bias\n",
            "rembert.encoder.layer.29.output.LayerNorm.weight\n",
            "rembert.encoder.layer.29.output.LayerNorm.bias\n",
            "rembert.encoder.layer.30.attention.self.query.weight\n",
            "rembert.encoder.layer.30.attention.self.query.bias\n",
            "rembert.encoder.layer.30.attention.self.key.weight\n",
            "rembert.encoder.layer.30.attention.self.key.bias\n",
            "rembert.encoder.layer.30.attention.self.value.weight\n",
            "rembert.encoder.layer.30.attention.self.value.bias\n",
            "rembert.encoder.layer.30.attention.output.dense.weight\n",
            "rembert.encoder.layer.30.attention.output.dense.bias\n",
            "rembert.encoder.layer.30.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.30.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.30.intermediate.dense.weight\n",
            "rembert.encoder.layer.30.intermediate.dense.bias\n",
            "rembert.encoder.layer.30.output.dense.weight\n",
            "rembert.encoder.layer.30.output.dense.bias\n",
            "rembert.encoder.layer.30.output.LayerNorm.weight\n",
            "rembert.encoder.layer.30.output.LayerNorm.bias\n",
            "rembert.encoder.layer.31.attention.self.query.weight\n",
            "rembert.encoder.layer.31.attention.self.query.bias\n",
            "rembert.encoder.layer.31.attention.self.key.weight\n",
            "rembert.encoder.layer.31.attention.self.key.bias\n",
            "rembert.encoder.layer.31.attention.self.value.weight\n",
            "rembert.encoder.layer.31.attention.self.value.bias\n",
            "rembert.encoder.layer.31.attention.output.dense.weight\n",
            "rembert.encoder.layer.31.attention.output.dense.bias\n",
            "rembert.encoder.layer.31.attention.output.LayerNorm.weight\n",
            "rembert.encoder.layer.31.attention.output.LayerNorm.bias\n",
            "rembert.encoder.layer.31.intermediate.dense.weight\n",
            "rembert.encoder.layer.31.intermediate.dense.bias\n",
            "rembert.encoder.layer.31.output.dense.weight\n",
            "rembert.encoder.layer.31.output.dense.bias\n",
            "rembert.encoder.layer.31.output.LayerNorm.weight\n",
            "rembert.encoder.layer.31.output.LayerNorm.bias\n",
            "rembert.pooler.dense.weight\n",
            "rembert.pooler.dense.bias\n",
            "classifier.weight\n",
            "classifier.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5f75cf7f",
      "metadata": {
        "id": "5f75cf7f"
      },
      "outputs": [],
      "source": [
        "# Freeze all of the pre-trained BERT layers to make the fine tuning go much faster.\n",
        "# Then later we'll try unfreezing some or all layers and see what works better.\n",
        "# We need to keep the final classification layer unfrozen no matter what,\n",
        "# because that's a new layer that hasn't been trained at all yet, and needs to be trained for our task.\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if name.split(\".\")[0] == \"bert\":\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6a08b323",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a08b323",
        "outputId": "823241f9-e32f-4036-d250-8742fc2b0dd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "RemBertForSequenceClassification                             --\n",
              "├─RemBertModel: 1-1                                          --\n",
              "│    └─RemBertEmbeddings: 2-1                                --\n",
              "│    │    └─Embedding: 3-1                                   64,076,800\n",
              "│    │    └─Embedding: 3-2                                   131,072\n",
              "│    │    └─Embedding: 3-3                                   512\n",
              "│    │    └─LayerNorm: 3-4                                   512\n",
              "│    │    └─Dropout: 3-5                                     --\n",
              "│    └─RemBertEncoder: 2-2                                   --\n",
              "│    │    └─Linear: 3-6                                      296,064\n",
              "│    │    └─ModuleList: 3-7                                  510,087,168\n",
              "│    └─RemBertPooler: 2-3                                    --\n",
              "│    │    └─Linear: 3-8                                      1,328,256\n",
              "│    │    └─Tanh: 3-9                                        --\n",
              "├─Dropout: 1-2                                               --\n",
              "├─Linear: 1-3                                                2,306\n",
              "=====================================================================================\n",
              "Total params: 575,922,690\n",
              "Trainable params: 575,922,690\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "# confirm all pre-trained layers are frozen\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "166cb951",
      "metadata": {
        "id": "166cb951"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8cf24dfe",
      "metadata": {
        "id": "8cf24dfe"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "num_epochs = 1\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"model_checkpoints\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True, # to help memory management\n",
        "    num_train_epochs=num_epochs,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "40b4ce70",
      "metadata": {
        "id": "40b4ce70"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Load the accuracy metric\n",
        "accuracy_metric = evaluate.load('accuracy')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # For single-label classification, use argmax to get predicted labels\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_metric.compute(predictions=predicted_labels, references=labels)\n",
        "    precision = precision_score(labels, predicted_labels, zero_division=0)\n",
        "    recall = recall_score(labels, predicted_labels, zero_division=0)\n",
        "    f1 = f1_score(labels, predicted_labels, zero_division=0)\n",
        "\n",
        "    # For AUC, we need the probabilities of the positive class (assuming binary classification)\n",
        "    # If predictions are logits, apply softmax or sigmoid to get probabilities\n",
        "    if predictions.shape[-1] == 2: # Check if it's a binary classification output\n",
        "        probabilities = torch.softmax(torch.tensor(predictions), dim=1)[:, 1].numpy()\n",
        "        auc = roc_auc_score(labels, probabilities)\n",
        "    else: # Handle cases that are not binary classification\n",
        "        auc = None # Or compute a different type of AUC if applicable\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': auc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YmnM-jRk-utP"
      },
      "id": "YmnM-jRk-utP",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ade2ed88",
      "metadata": {
        "id": "ade2ed88"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "THo9uVat_vmo"
      },
      "id": "THo9uVat_vmo",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HbfLySN_8Jx",
        "outputId": "16f2b172-4903-4761-b20e-34def67063f9"
      },
      "id": "8HbfLySN_8Jx",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 16 20:50:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0             26W /   70W |    2620MiB /  15360MiB |      4%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1fde0f66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "1fde0f66",
        "outputId": "825d6186-54ac-4cb0-c183-2fbca2014f21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15351' max='15351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15351/15351 1:49:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Roc Auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.692401</td>\n",
              "      <td>0.519098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.502139</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15351, training_loss=0.7014112365058669, metrics={'train_runtime': 6542.4959, 'train_samples_per_second': 4.693, 'train_steps_per_second': 2.346, 'total_flos': 6032890607159808.0, 'train_loss': 0.7014112365058669, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dM_EJf20n8MT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "dM_EJf20n8MT",
        "outputId": "b73e91b6-ddb2-44c1-aff1-82c1ade3c8ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1951' max='1951' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1951/1951 01:23]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6924005746841431,\n",
              " 'eval_accuracy': 0.5190976672648039,\n",
              " 'eval_precision': 0.0,\n",
              " 'eval_recall': 0.0,\n",
              " 'eval_f1': 0.0,\n",
              " 'eval_roc_auc': 0.5021394350996341,\n",
              " 'eval_runtime': 84.0387,\n",
              " 'eval_samples_per_second': 46.419,\n",
              " 'eval_steps_per_second': 23.215,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "sKh2wt6SyoGM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKh2wt6SyoGM",
        "outputId": "997eb8c6-c56c-467d-f83f-6ae3e9ed31a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: [CLS] En marche ou crÃ ̈ve ![SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n",
            "True Hatespeech Label: 0\n",
            "Predicted Hatespeech Label: 0\n",
            "Hatespeech Probabilities: [[0.5157418251037598, 0.48425814509391785]]\n"
          ]
        }
      ],
      "source": [
        "# Get a sample from the validation dataset\n",
        "sample_index = 2500\n",
        "val_sample = val_dataset[sample_index]\n",
        "\n",
        "# Prepare the input for the model\n",
        "inputs = {\n",
        "    'input_ids': torch.tensor([val_sample['input_ids']]),\n",
        "    'attention_mask': torch.tensor([val_sample['attention_mask']]),\n",
        "    'token_type_ids': torch.tensor([val_sample['token_type_ids']]) # Added token_type_ids\n",
        "}\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = model.device\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Get the model prediction\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Access the correct logits from the dictionary output and calculate probabilities\n",
        "# hatespeech_logits = outputs['hatespeech_logits'] # Removed incorrect key\n",
        "logits = outputs['logits'] # Accessed logits using the correct key\n",
        "hatespeech_probabilities = torch.softmax(logits, dim=1)\n",
        "predicted_hatespeech_label = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "print(f\"Text: {tokenizer.decode(val_sample['input_ids'])}\")\n",
        "print(f\"True Hatespeech Label: {val_sample['labels']}\") # Corrected label access\n",
        "print(f\"Predicted Hatespeech Label: {predicted_hatespeech_label}\")\n",
        "print(f\"Hatespeech Probabilities: {hatespeech_probabilities.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "n0FsVNJLzSQE",
      "metadata": {
        "id": "n0FsVNJLzSQE"
      },
      "outputs": [],
      "source": [
        "true_hatespeech_labels = []\n",
        "predicted_hatespeech_labels = []\n",
        "\n",
        "device = model.device\n",
        "\n",
        "for val_sample in val_dataset:\n",
        "    inputs = {\n",
        "        'input_ids': torch.tensor([val_sample['input_ids']]).to(device),\n",
        "        'attention_mask': torch.tensor([val_sample['attention_mask']]).to(device)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    hatespeech_logits = outputs['logits']\n",
        "    predicted_hatespeech_label = torch.argmax(hatespeech_logits, dim=1).item()\n",
        "\n",
        "    true_hatespeech_labels.append(val_sample['labels'])\n",
        "    predicted_hatespeech_labels.append(predicted_hatespeech_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "FlARFhjZzOI3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "FlARFhjZzOI3",
        "outputId": "3eaa3ab7-c026-48d2-e16d-80f65aee4535"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "cm = confusion_matrix(true_hatespeech_labels, predicted_hatespeech_labels)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for Hatespeech Prediction')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b477aeb",
      "metadata": {
        "id": "4b477aeb"
      },
      "source": [
        "## Multi-Label Prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "wkg61qLz_q0h",
        "outputId": "87506392-4f32-44c4-9f57-a3134de33431"
      },
      "id": "wkg61qLz_q0h",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stop' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-3957423419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92eL8T6DAYiu",
      "metadata": {
        "id": "92eL8T6DAYiu"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "example = train_dataset[0]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4358b5f9",
      "metadata": {
        "id": "4358b5f9"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yyHWaVa5D7VT",
      "metadata": {
        "id": "yyHWaVa5D7VT"
      },
      "outputs": [],
      "source": [
        "example = dataset['train'][13000]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e692717d",
      "metadata": {
        "id": "e692717d"
      },
      "outputs": [],
      "source": [
        "# Original binary labels\n",
        "binary_label_columns = [\n",
        "    'hatespeech',\n",
        "    'sentiment_indifference',\n",
        "    'sentiment_disgust',\n",
        "    'sentiment_sadness',\n",
        "    'sentiment_anger',\n",
        "    'sentiment_shock',\n",
        "    'sentiment_normal',\n",
        "    'sentiment_confusion',\n",
        "    'sentiment_fear'\n",
        "]\n",
        "\n",
        "# Get unique values for multiclass labels to determine the range for one-hot encoding\n",
        "directness_values = sorted(train_df['directness_label'].unique().tolist())\n",
        "target_values = sorted(train_df['target_label'].unique().tolist())\n",
        "group_values = sorted(train_df['group_label'].unique().tolist())\n",
        "\n",
        "# Create names for one-hot encoded binary labels\n",
        "directness_binary_labels = [f'directness_label_{val}' for val in directness_values]\n",
        "target_binary_labels = [f'target_label_{val}' for val in target_values]\n",
        "group_binary_labels = [f'group_label_{val}' for val in group_values]\n",
        "\n",
        "# Combine all binary label names\n",
        "all_binary_labels = binary_label_columns + directness_binary_labels + target_binary_labels + group_binary_labels\n",
        "\n",
        "# Update id2label and label2id\n",
        "id2label = {idx:label for idx, label in enumerate(all_binary_labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(all_binary_labels)}\n",
        "\n",
        "# The total number of binary labels\n",
        "num_binary_labels = len(all_binary_labels)\n",
        "\n",
        "print(f\"Total number of binary labels: {num_binary_labels}\")\n",
        "print(f\"All binary labels: {all_binary_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GfuEZFkluScv",
      "metadata": {
        "id": "GfuEZFkluScv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-multilingual-cased\",\n",
        "                                                           problem_type=\"multi_label_classification\",\n",
        "                                                           num_labels=len(all_binary_labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LLuk7TLNyE6r",
      "metadata": {
        "id": "LLuk7TLNyE6r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Get unique values for multiclass labels to determine the range for one-hot encoding\n",
        "directness_values = sorted(train_df['directness_label'].unique().tolist())\n",
        "target_values = sorted(train_df['target_label'].unique().tolist())\n",
        "group_values = sorted(train_df['group_label'].unique().tolist())\n",
        "\n",
        "def preprocess_multioutput_data(examples):\n",
        "  # take a batch of texts\n",
        "  text = [str(x) for x in examples[\"text_cleaned\"]]\n",
        "  # encode them, returning PyTorch tensors directly\n",
        "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=150, return_tensors=\"pt\")\n",
        "\n",
        "  # Create list of binary label tensors\n",
        "  binary_labels_list = [\n",
        "      torch.tensor(examples[\"hatespeech\"]),\n",
        "      torch.tensor(examples[\"sentiment_anger\"]),\n",
        "      torch.tensor(examples[\"sentiment_confusion\"]),\n",
        "      torch.tensor(examples[\"sentiment_disgust\"]),\n",
        "      torch.tensor(examples[\"sentiment_fear\"]),\n",
        "      torch.tensor(examples[\"sentiment_indifference\"]),\n",
        "      torch.tensor(examples[\"sentiment_normal\"]),\n",
        "      torch.tensor(examples[\"sentiment_sadness\"]),\n",
        "      torch.tensor(examples[\"sentiment_shock\"])\n",
        "  ]\n",
        "\n",
        "  # Perform one-hot encoding for multiclass labels and add to the list\n",
        "  for i, val in enumerate(directness_values):\n",
        "      binary_labels_list.append(torch.tensor([1.0 if label == val else 0.0 for label in examples[\"directness_label\"]]))\n",
        "\n",
        "  for i, val in enumerate(target_values):\n",
        "      binary_labels_list.append(torch.tensor([1.0 if label == val else 0.0 for label in examples[\"target_label\"]]))\n",
        "\n",
        "  for i, val in enumerate(group_values):\n",
        "      binary_labels_list.append(torch.tensor([1.0 if label == val else 0.0 for label in examples[\"group_label\"]]))\n",
        "\n",
        "\n",
        "  # Stack all binary label tensors along dimension 1\n",
        "  labels_tensor = torch.stack(binary_labels_list, dim=1).float()\n",
        "\n",
        "  # Add the stacked labels tensor to the encoding dictionary\n",
        "  encoding['labels'] = labels_tensor\n",
        "\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X-j-plQeyI4z",
      "metadata": {
        "id": "X-j-plQeyI4z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Apply the preprocessing function with batching\n",
        "encoded_dataset = dataset.map(preprocess_multioutput_data, batched=True, remove_columns=dataset['train'].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rkdRwF_eyQP7",
      "metadata": {
        "id": "rkdRwF_eyQP7"
      },
      "outputs": [],
      "source": [
        "example = encoded_dataset['train'][13000]\n",
        "print(example.keys())\n",
        "\n",
        "print(tokenizer.decode(example['input_ids']))\n",
        "\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aYo5T7Uuz4Z",
      "metadata": {
        "id": "0aYo5T7Uuz4Z"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vYNoPAf-uvYG",
      "metadata": {
        "id": "vYNoPAf-uvYG"
      },
      "outputs": [],
      "source": [
        "# Freeze all of the pre-trained BERT layers to make the fine tuning go much faster.\n",
        "# Then later we'll try unfreezing some or all layers and see what works better.\n",
        "# We need to keep the final classification layer unfrozen no matter what,\n",
        "# because that's a new layer that hasn't been trained at all yet, and needs to be trained for our task.\n",
        "\n",
        "\n",
        "for name, param in model.distilbert.named_parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wthrZy4Au1AE",
      "metadata": {
        "id": "wthrZy4Au1AE"
      },
      "outputs": [],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SRCZgZAa4QmL",
      "metadata": {
        "id": "SRCZgZAa4QmL"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "batch_size = 64\n",
        "metric_name = \"eval_f1\" # Changed to monitor a specific per-label F1\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-base-multilingual-cased\",\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True, # to help memory management\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    # Use the micro-averaged F1 as the metric to monitor\n",
        "    metric_for_best_model=metric_name,\n",
        "    greater_is_better=True, # F1 score is better when higher\n",
        "    report_to='none' # Disable reporting to wandb for this example\n",
        ")\n",
        "\n",
        "# Add EarlyStoppingCallback\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3, # Number of evaluation epochs to wait before stopping\n",
        "    early_stopping_threshold=0.0 # Minimum improvement to consider as progress\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rjk99d1E4ua0",
      "metadata": {
        "id": "Rjk99d1E4ua0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score, balanced_accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels.astype(int) # Ensure labels are integers\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    balanced_accuracy = balanced_accuracy_score(y_true.flatten(), y_pred.flatten())\n",
        "    precision = precision_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
        "    recall = recall_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
        "    # return as dictionary\n",
        "    metrics = {'f1': f1_micro_average,\n",
        "               'roc_auc': roc_auc,\n",
        "               'accuracy': accuracy,\n",
        "               'balanced_accuracy': balanced_accuracy,\n",
        "               'precision': precision,\n",
        "               'recall': recall}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions,\n",
        "            tuple) else p.predictions\n",
        "\n",
        "    # Ensure labels are a 2D numpy array of integers\n",
        "    labels_np = np.array(p.label_ids)\n",
        "    if labels_np.ndim == 1:\n",
        "        # This might happen if the batch size is 1, reshape to (1, num_labels)\n",
        "        labels_np = labels_np.reshape(1, -1)\n",
        "    elif labels_np.ndim > 2:\n",
        "        # Handle cases where labels might have extra dimensions unexpectedly\n",
        "        # This might require more specific reshaping based on the actual structure\n",
        "        # For now, let's assume the expected shape is (batch_size, num_labels)\n",
        "        # and try to flatten and reshape if necessary.\n",
        "        # A safer approach might be to raise an error or log a warning here.\n",
        "        print(f\"Warning: Unexpected label dimension: {labels_np.ndim}\")\n",
        "        # Attempt to flatten and reshape - adjust if your data structure is different\n",
        "        labels_np = labels_np.reshape(labels_np.shape[0], -1)\n",
        "\n",
        "\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds,\n",
        "        labels=labels_np)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9elP4nZpvMjf",
      "metadata": {
        "id": "9elP4nZpvMjf"
      },
      "outputs": [],
      "source": [
        "#forward pass\n",
        "# # Check the type of the data after setting the format\n",
        "# print(f\"Type of input_ids[0]: {encoded_dataset['train']['input_ids'][0].type()}\")\n",
        "# print(f\"Type of labels[0]: {encoded_dataset['train'][0]['hatespeech_labels'].type()}\")\n",
        "\n",
        "# Also print a sample example to see the structure\n",
        "print(\"Sample example after setting format:\")\n",
        "print(encoded_dataset['train'][0])\n",
        "\n",
        "# Manually get a sample and prepare input for the model\n",
        "sample = encoded_dataset['train'][0]\n",
        "\n",
        "# Convert lists to PyTorch tensors before unsqueezing\n",
        "input_ids_tensor = torch.tensor(sample['input_ids']).unsqueeze(0)\n",
        "attention_mask_tensor = torch.tensor(sample['attention_mask']).unsqueeze(0)\n",
        "labels_tensor = torch.tensor(sample['labels']).unsqueeze(0)\n",
        "\n",
        "# Move tensors to the same device as the model\n",
        "device = model.device\n",
        "input_ids_tensor = input_ids_tensor.to(device)\n",
        "attention_mask_tensor = attention_mask_tensor.to(device)\n",
        "labels_tensor = labels_tensor.to(device)\n",
        "\n",
        "\n",
        "outputs = model(input_ids=input_ids_tensor, attention_mask=attention_mask_tensor, labels=labels_tensor)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CucouWT25Sdd",
      "metadata": {
        "id": "CucouWT25Sdd"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "io-bvKASvUm6",
      "metadata": {
        "id": "io-bvKASvUm6"
      },
      "outputs": [],
      "source": [
        "print(encoded_dataset['train']['labels'][13000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9iCETiVvZVV",
      "metadata": {
        "id": "D9iCETiVvZVV"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YROJw66Svbe6",
      "metadata": {
        "id": "YROJw66Svbe6"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mZxlPhNWvlmz",
      "metadata": {
        "id": "mZxlPhNWvlmz"
      },
      "outputs": [],
      "source": [
        "text = \"I'm happy I can finally train a model for multi-label classification\"\n",
        "\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")\n",
        "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
        "\n",
        "outputs = trainer.model(**encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W7i4y2F_vnKR",
      "metadata": {
        "id": "W7i4y2F_vnKR"
      },
      "outputs": [],
      "source": [
        "logits = outputs.logits\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L_RUl6L-voQD",
      "metadata": {
        "id": "L_RUl6L-voQD"
      },
      "outputs": [],
      "source": [
        "# apply sigmoid + threshold\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "probs = sigmoid(logits.squeeze().cpu())\n",
        "predictions = np.zeros(probs.shape)\n",
        "predictions[np.where(probs >= 0.5)] = 1\n",
        "# turn predicted id's into actual label names\n",
        "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
        "print(\"Predicted labels (names):\", predicted_labels)\n",
        "print(\"Predicted labels (binary values):\", predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zqG_BSeFvrma",
      "metadata": {
        "id": "zqG_BSeFvrma"
      },
      "outputs": [],
      "source": [
        "true_labels = []\n",
        "predicted_logits = []\n",
        "\n",
        "device = trainer.model.device\n",
        "\n",
        "for sample in encoded_dataset[\"validation\"]:\n",
        "    inputs = {\n",
        "        'input_ids': torch.tensor([sample['input_ids']]).to(device),\n",
        "        'attention_mask': torch.tensor([sample['attention_mask']]).to(device)\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = trainer.model(**inputs)\n",
        "\n",
        "    # Append the true labels and predicted logits\n",
        "    true_labels.append(sample['labels'])\n",
        "    predicted_logits.append(outputs.logits.squeeze().cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lAosIonevuJf",
      "metadata": {
        "id": "lAosIonevuJf"
      },
      "outputs": [],
      "source": [
        "# Convert lists to numpy arrays\n",
        "true_labels_np = np.array(true_labels)\n",
        "predicted_logits_np = np.array(predicted_logits)\n",
        "\n",
        "# Define indices for each label group based on all_binary_labels\n",
        "hatespeech_idx = all_binary_labels.index('hatespeech')\n",
        "sentiment_indices = [all_binary_labels.index(label) for label in binary_label_columns if 'sentiment' in label]\n",
        "directness_indices = [all_binary_labels.index(label) for label in directness_binary_labels]\n",
        "target_indices = [all_binary_labels.index(label) for label in target_binary_labels]\n",
        "group_indices = [all_binary_labels.index(label) for label in group_binary_labels]\n",
        "\n",
        "\n",
        "# Split true labels and predicted logits\n",
        "true_hatespeech = true_labels_np[:, hatespeech_idx]\n",
        "predicted_hatespeech_logits = predicted_logits_np[:, hatespeech_idx]\n",
        "\n",
        "true_sentiment_labels = true_labels_np[:, sentiment_indices]\n",
        "predicted_sentiment_logits = predicted_logits_np[:, sentiment_indices]\n",
        "\n",
        "true_directness_labels = true_labels_np[:, directness_indices]\n",
        "predicted_directness_logits = predicted_logits_np[:, directness_indices]\n",
        "\n",
        "true_target_labels = true_labels_np[:, target_indices]\n",
        "predicted_target_logits = predicted_logits_np[:, target_indices]\n",
        "\n",
        "true_group_labels = true_labels_np[:, group_indices]\n",
        "predicted_group_logits = predicted_logits_np[:, group_indices]\n",
        "\n",
        "print(\"True Hatespeech Labels shape:\", true_hatespeech.shape)\n",
        "print(\"Predicted Hatespeech Logits shape:\", predicted_hatespeech_logits.shape)\n",
        "print(\"True Sentiment Labels shape:\", true_sentiment_labels.shape)\n",
        "print(\"Predicted Sentiment Logits shape:\", predicted_sentiment_logits.shape)\n",
        "print(\"True Directness Labels shape:\", true_directness_labels.shape)\n",
        "print(\"Predicted Directness Logits shape:\", predicted_directness_logits.shape)\n",
        "print(\"True Target Labels shape:\", true_target_labels.shape)\n",
        "print(\"Predicted Target Logits shape:\", predicted_target_logits.shape)\n",
        "print(\"True Group Labels shape:\", true_group_labels.shape)\n",
        "print(\"Predicted Group Logits shape:\", predicted_group_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ACn91_XDvvjA",
      "metadata": {
        "id": "ACn91_XDvvjA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def calculate_metrics_and_confusion_matrix(true_labels, predicted_logits, threshold=0.5, is_multilabel=True):\n",
        "    \"\"\"\n",
        "    Calculates classification metrics and confusion matrix.\n",
        "\n",
        "    Args:\n",
        "        true_labels (np.ndarray): True labels (binary).\n",
        "        predicted_logits (np.ndarray): Predicted logits from the model.\n",
        "        threshold (float): Threshold to convert probabilities to binary predictions.\n",
        "        is_multilabel (bool): Whether the task is multi-label classification.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing accuracy, precision, recall, and F1 score.\n",
        "        np.ndarray: Confusion matrix.\n",
        "    \"\"\"\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predicted_logits)).numpy()\n",
        "\n",
        "    if is_multilabel:\n",
        "        # For multi-label, apply threshold to each label independently\n",
        "        predicted_labels = (probs >= threshold).astype(int)\n",
        "    else:\n",
        "        # For binary (hatespeech), apply threshold to the single output\n",
        "        predicted_labels = (probs >= threshold).astype(int)\n",
        "\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels.flatten(), predicted_labels.flatten())\n",
        "    balanced_accuracy = balanced_accuracy_score(true_labels.flatten(), predicted_labels.flatten())\n",
        "    precision = precision_score(true_labels.flatten(), predicted_labels.flatten(), average='micro', zero_division=0)\n",
        "    recall = recall_score(true_labels.flatten(), predicted_labels.flatten(), average='micro', zero_division=0)\n",
        "    f1 = f1_score(true_labels.flatten(), predicted_labels.flatten(), average='micro', zero_division=0)\n",
        "\n",
        "    # Calculate confusion matrix - Note: For multi-label, CM is often calculated per label or overall\n",
        "    # A simple overall CM for multilabel can be misleading. We'll calculate it based on flattened arrays\n",
        "    # which treats each label prediction as a separate instance.\n",
        "    cm = confusion_matrix(true_labels.flatten(), predicted_labels.flatten())\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'balanced_accuracy': balanced_accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "    return metrics, cm\n",
        "\n",
        "# Calculate metrics and confusion matrices for each group\n",
        "\n",
        "# Hatespeech (Binary Classification)\n",
        "hatespeech_metrics, hatespeech_cm = calculate_metrics_and_confusion_matrix(\n",
        "    true_hatespeech, predicted_hatespeech_logits, is_multilabel=False\n",
        ")\n",
        "print(\"Hatespeech Metrics:\", hatespeech_metrics)\n",
        "print(\"Hatespeech Confusion Matrix:\\n\", hatespeech_cm)\n",
        "\n",
        "\n",
        "# Sentiment (Multi-label Classification)\n",
        "sentiment_metrics, sentiment_cm = calculate_metrics_and_confusion_matrix(\n",
        "    true_sentiment_labels, predicted_sentiment_logits, is_multilabel=True\n",
        ")\n",
        "print(\"\\nSentiment Metrics (Micro Avg):\", sentiment_metrics)\n",
        "print(\"Sentiment Confusion Matrix (Flattened):\\n\", sentiment_cm)\n",
        "\n",
        "\n",
        "# Directness (Multi-label Classification - treated as binary for each category)\n",
        "directness_metrics, directness_cm = calculate_metrics_and_confusion_matrix(\n",
        "    true_directness_labels, predicted_directness_logits, is_multilabel=True\n",
        ")\n",
        "print(\"\\nDirectness Metrics (Micro Avg):\", directness_metrics)\n",
        "print(\"Directness Confusion Matrix (Flattened):\\n\", directness_cm)\n",
        "\n",
        "# Target (Multi-label Classification - treated as binary for each category)\n",
        "target_metrics, target_cm = calculate_metrics_and_confusion_matrix(\n",
        "    true_target_labels, predicted_target_logits, is_multilabel=True\n",
        ")\n",
        "print(\"\\nTarget Metrics (Micro Avg):\", target_metrics)\n",
        "print(\"Target Confusion Matrix (Flattened):\\n\", target_cm)\n",
        "\n",
        "# Group (Multi-label Classification - treated as binary for each category)\n",
        "group_metrics, group_cm = calculate_metrics_and_confusion_matrix(\n",
        "    true_group_labels, predicted_group_logits, is_multilabel=True\n",
        ")\n",
        "print(\"\\nGroup Metrics (Micro Avg):\", group_metrics)\n",
        "print(\"Group Confusion Matrix (Flattened):\\n\", group_cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wHApR61Tvziu",
      "metadata": {
        "id": "wHApR61Tvziu"
      },
      "outputs": [],
      "source": [
        "# Visualize Flattened Confusion Matrices for each group\n",
        "\n",
        "# Hatespeech Flattened Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(hatespeech_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Flattened Confusion Matrix for: Hatespeech')\n",
        "plt.show()\n",
        "\n",
        "# Sentiment Flattened Confusion Matrix (Micro Avg across labels)\n",
        "# Note: This matrix is based on the flattened true and predicted labels across all sentiment types.\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(sentiment_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Flattened Confusion Matrix for: Sentiment (Micro Avg)')\n",
        "plt.show()\n",
        "\n",
        "# Directness Flattened Confusion Matrix (Micro Avg across categories)\n",
        "# Note: This matrix is based on the flattened true and predicted labels across all directness categories.\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(directness_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Flattened Confusion Matrix for: Directness (Micro Avg)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Target Flattened Confusion Matrix (Micro Avg across categories)\n",
        "# Note: This matrix is based on the flattened true and predicted labels across all target categories.\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(target_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Flattened Confusion Matrix for: Target (Micro Avg)')\n",
        "plt.show()\n",
        "\n",
        "# Group Flattened Confusion Matrix (Micro Avg across categories)\n",
        "# Note: This matrix is based on the flattened true and predicted labels across all group categories.\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(group_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Flattened Confusion Matrix for: Group (Micro Avg)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1MBy7FgS4Xze",
      "metadata": {
        "id": "1MBy7FgS4Xze"
      },
      "outputs": [],
      "source": [
        "# Visualize Confusion Matrices for each individual binary label\n",
        "\n",
        "# Apply sigmoid to get probabilities\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "predicted_probs_np = sigmoid(torch.Tensor(predicted_logits_np)).numpy()\n",
        "\n",
        "# Apply threshold to get binary predictions for each label\n",
        "predicted_binary_labels_np = (predicted_probs_np >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "# Iterate through each binary label and plot its confusion matrix\n",
        "for i, label_name in id2label.items():\n",
        "    true_binary = true_labels_np[:, i]\n",
        "    predicted_binary = predicted_binary_labels_np[:, i]\n",
        "\n",
        "    # Calculate confusion matrix for the current binary label\n",
        "    cm = confusion_matrix(true_binary, predicted_binary)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['True 0', 'True 1'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f'Confusion Matrix for: {label_name}')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9c625d67645461b8091bcdcb2163c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0e4046d593c4beeb20fc95006b937f4",
              "IPY_MODEL_1862d5d1184840f785764c4f7a495bba",
              "IPY_MODEL_e94de0dda93b4b78a1dda3a23cb88b73"
            ],
            "layout": "IPY_MODEL_6990060972504539bfffeca0d73ee1ba"
          }
        },
        "e0e4046d593c4beeb20fc95006b937f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad7a77b4cbe3422ba0462ea6883464e5",
            "placeholder": "​",
            "style": "IPY_MODEL_1912db96fdfc4d779fb5af692ad0c13d",
            "value": "Map: 100%"
          }
        },
        "1862d5d1184840f785764c4f7a495bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5d83aaa0814f3195bc9e89f9b0ab3d",
            "max": 30702,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dff4a2dd3d549f1a01e812baa248aa8",
            "value": 30702
          }
        },
        "e94de0dda93b4b78a1dda3a23cb88b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64b2426a1919496185189f618f897abd",
            "placeholder": "​",
            "style": "IPY_MODEL_9a037da8941d4f7588ee8cd1ec9caa52",
            "value": " 30702/30702 [00:10&lt;00:00, 1909.33 examples/s]"
          }
        },
        "6990060972504539bfffeca0d73ee1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad7a77b4cbe3422ba0462ea6883464e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1912db96fdfc4d779fb5af692ad0c13d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de5d83aaa0814f3195bc9e89f9b0ab3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dff4a2dd3d549f1a01e812baa248aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64b2426a1919496185189f618f897abd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a037da8941d4f7588ee8cd1ec9caa52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a69b75bb08b4105a6d80001f05a8bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3333f29358af4ae58e496be16e81675a",
              "IPY_MODEL_2c1f2590d17849bcb44493af51f8fa70",
              "IPY_MODEL_0024d592205c4f908a4d7def0a05d31e"
            ],
            "layout": "IPY_MODEL_4883242d6af14ad18700551be6112f4b"
          }
        },
        "3333f29358af4ae58e496be16e81675a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13d6ea4a1e0c453d81d7ce9d758106ab",
            "placeholder": "​",
            "style": "IPY_MODEL_de7ceda7db26473a93ddd5300e2aa602",
            "value": "Map: 100%"
          }
        },
        "2c1f2590d17849bcb44493af51f8fa70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51c7b39ca50452f8e1da11149fbb6d7",
            "max": 3901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b729ca6a1ca47629751b00e3a3ead8c",
            "value": 3901
          }
        },
        "0024d592205c4f908a4d7def0a05d31e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d39b58f10b124979899dcd3b4e34bee8",
            "placeholder": "​",
            "style": "IPY_MODEL_48d8a25eb8184c60a3b49da2b851287e",
            "value": " 3901/3901 [00:03&lt;00:00, 1379.49 examples/s]"
          }
        },
        "4883242d6af14ad18700551be6112f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13d6ea4a1e0c453d81d7ce9d758106ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7ceda7db26473a93ddd5300e2aa602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a51c7b39ca50452f8e1da11149fbb6d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b729ca6a1ca47629751b00e3a3ead8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d39b58f10b124979899dcd3b4e34bee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d8a25eb8184c60a3b49da2b851287e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}